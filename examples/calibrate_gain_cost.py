#!/usr/bin/env python3
"""
Gain-Cost Model Parameter Calibration v2
æŒ‰ç…§ç”¨æˆ·å»ºè®®çš„ç§‘å­¦æµ‹è¯•æ–¹æ³•ï¼š
- Round 1: åŸºå‡†æµ‹è¯• (1E + 0P + 2D, æ³Šæ¾åˆ†å¸ƒ, 50 requests)
- Round 2: æŠ¢å æµ‹è¯• (1E + 2P + 1D, PrefillæŠ¢å Decode)
- Round 3: å³°å€¼å‹æµ‹ (1E + 2P + 1D, æœ€å¤§token budget)
"""

import os
import asyncio
import numpy as np
import time
import json
import argparse
import ray
from typing import List, Dict, Any
from pathlib import Path

from elasticmm.engine.v0.backend import V0EngineBackend
from elasticmm.engine.v0.utils import Request, EngineStage
from elasticmm.core.balancer import ModalityType


class PoissonRequestGenerator:
    """æ³Šæ¾åˆ†å¸ƒè¯·æ±‚ç”Ÿæˆå™¨"""
    
    def __init__(self, arrival_rate: float = 2.0, total_requests: int = 50):
        """
        Args:
            arrival_rate: å¹³å‡æ¯ç§’åˆ°è¾¾çš„è¯·æ±‚æ•° (lambda)
            total_requests: æ€»è¯·æ±‚æ•°
        """
        self.arrival_rate = arrival_rate
        self.total_requests = total_requests
    
    def generate_arrival_times(self) -> List[float]:
        """ç”Ÿæˆè¯·æ±‚åˆ°è¾¾æ—¶é—´ (ç›¸å¯¹äºå¼€å§‹æ—¶é—´çš„åç§»ï¼Œå•ä½ï¼šç§’)"""
        # æ³Šæ¾è¿‡ç¨‹ï¼šé—´éš”æ—¶é—´æœä»æŒ‡æ•°åˆ†å¸ƒ
        intervals = np.random.exponential(1.0 / self.arrival_rate, self.total_requests)
        arrival_times = np.cumsum(intervals)
        return arrival_times.tolist()
    
    def create_test_requests(self, images: List[Any]) -> List[Request]:
        """åˆ›å»ºæµ‹è¯•è¯·æ±‚ï¼ˆå¸¦å›¾ç‰‡çš„å¤šæ¨¡æ€è¯·æ±‚ï¼‰"""
        from PIL import Image
        
        prompts = [
            "<|user|>\n<|vision_start|><|image_pad|><|vision_end|>\nè¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚\n<|assistant|>\n",
            "<|user|>\n<|vision_start|><|image_pad|><|vision_end|>\nå›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ\n<|assistant|>\n",
            "<|user|>\n<|vision_start|><|image_pad|><|vision_end|>\nè¿™æ˜¯ä»€ä¹ˆåœºæ™¯ï¼Ÿ\n<|assistant|>\n",
            "<|user|>\n<|vision_start|><|image_pad|><|vision_end|>\nè¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ã€‚\n<|assistant|>\n",
        ]
        
        requests = []
        for i in range(self.total_requests):
            img = images[i % len(images)]  # å¾ªç¯ä½¿ç”¨å›¾ç‰‡
            prompt = prompts[i % len(prompts)]  # å¾ªç¯ä½¿ç”¨prompts
            
            req = Request(
                request_id=f"poisson_req_{i:03d}",
                prompt=prompt,
                max_tokens=20,  # å›ºå®šè¾“å‡ºé•¿åº¦
                temperature=0.8,
                top_p=0.9,
                multi_modal_data={"image": img},
            )
            requests.append(req)
        return requests


class GainCostCalibrator:
    """Gain-Costæ¨¡å‹å‚æ•°æ ¡å‡†å™¨"""
    
    def __init__(self, model_path: str, output_dir: str = "./calibration_results"):
        self.model_path = model_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # æ ¡å‡†ç»“æœ
        self.results = {
            'round1_baseline': {},
            'round2_preemption': {},
            'round3_peak': {},
            'final_params': {},
        }
    
    async def round1_baseline(self):
        """
        Round 1: åŸºå‡†æµ‹è¯•
        é…ç½®: 1E + 1P + 2D
        è´Ÿè½½: 50 requests, æ³Šæ¾åˆ†å¸ƒ (Î»=2 req/s)
        æµ‹é‡: decodeè¾“å‡ºå»¶è¿Ÿ, KVä¼ è¾“å¸¦å®½
        """
        print("\n" + "="*80)
        print("ğŸ”¬ Round 1: åŸºå‡†æµ‹è¯• (Baseline)")
        print("="*80)
        print("é…ç½®: 1 Encoding + 1 Prefill + 2 Decoding")
        print("è´Ÿè½½: 50 requests, æ³Šæ¾åˆ†å¸ƒ (Î»=2 req/s)")
        print("ç›®æ ‡: æµ‹é‡prefill/decodeå»¶è¿Ÿ(ms/token), KVä¼ è¾“å¸¦å®½")
        print("="*80)
        
        # åˆå§‹åŒ–backend
        backend = V0EngineBackend(
            model_path=self.model_path,
            num_encoding_workers=1,
            num_prefill_workers=1,
            num_decoding_workers=2,
            block_size=16,
            max_num_gpu_blocks=3000,
            dtype="float16",
            gpu_memory_utilization=0.85,
            kv_transfer_method="nccl",
            limit_mm_per_prompt={"image": 1},
        )
        
        print("\n[Round 1] Initializing backend...")
        await backend.initialize()
        
        print("[Round 1] Starting backend...")
        await backend.start()
        await asyncio.sleep(2)  # ç­‰å¾…ç¨³å®š
        
        # åŠ è½½æµ‹è¯•å›¾ç‰‡
        print("\n[Round 1] Loading test images...")
        from PIL import Image
        image_base_path = "/root/lzd/dataset/shareGPT-4o/images"
        
        images = []
        for i in range(10, 30):  # åŠ è½½20å¼ å›¾ç‰‡
            image_path = f"{image_base_path}/{i}.jpg"
            try:
                img = Image.open(image_path)
                images.append(img)
            except Exception as e:
                print(f"  âš  Failed to load {image_path}, using dummy image")
                import numpy as np
                img_array = np.full((224, 224, 3), (128, 128, 128), dtype=np.uint8)
                img = Image.fromarray(img_array)
                images.append(img)
        
        print(f"[Round 1] Loaded {len(images)} images")
        
        # ç”Ÿæˆè¯·æ±‚
        print("\n[Round 1] Generating Poisson-distributed requests...")
        generator = PoissonRequestGenerator(arrival_rate=2.0, total_requests=50)
        requests = generator.create_test_requests(images)
        arrival_times = generator.generate_arrival_times()
        
        print(f"[Round 1] Generated {len(requests)} multimodal requests")
        print(f"[Round 1] Expected duration: ~{arrival_times[-1]:.1f}s")
        
        # æäº¤è¯·æ±‚
        start_time = time.time()
        completed_requests = []
        
        async def submit_requests():
            for req, arrival_time in zip(requests, arrival_times):
                # ç­‰å¾…åˆ°è¾¾æ—¶é—´
                current_time = time.time() - start_time
                wait_time = arrival_time - current_time
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
                
                await backend.add_request(req)
                print(f"[Round 1] Submitted {req.request_id} at t={time.time()-start_time:.2f}s")
        
        async def collect_outputs():
            while len(completed_requests) < len(requests):
                outputs = await backend.get_outputs()
                for output in outputs:
                    if output.finished:
                        completed_requests.append(output)
                        print(f"[Round 1] Completed {output.request_id} ({len(completed_requests)}/{len(requests)})")
                await asyncio.sleep(0.1)
        
        # å¹¶å‘æ‰§è¡Œæäº¤å’Œæ”¶é›†
        await asyncio.gather(
            submit_requests(),
            collect_outputs(),
        )
        
        total_time = time.time() - start_time
        print(f"\n[Round 1] All requests completed in {total_time:.2f}s")
        
        # è·å–æ€§èƒ½æŒ‡æ ‡
        print("\n[Round 1] Collecting performance metrics...")
        perf_metrics = backend.get_performance_metrics()
        
        # è®¡ç®—ç»“æœ
        results = {
            'config': '1E + 1P + 2D',
            'num_requests': len(requests),
            'total_time': total_time,
            'throughput': len(requests) / total_time,
            'metrics': perf_metrics,
        }
        
        # æå–å…³é”®æŒ‡æ ‡
        if 'decoding' in perf_metrics and 'decode_latency_ms_per_token' in perf_metrics['decoding']:
            decode_latency = perf_metrics['decoding']['decode_latency_ms_per_token']
            results['decode_ms_per_token'] = decode_latency.get('mean', 0)
            print(f"\nğŸ“Š Decodeå»¶è¿Ÿ (å½’ä¸€åŒ–): {decode_latency.get('mean', 0):.3f} ms/token")
            print(f"   - P50: {decode_latency.get('p50', 0):.3f} ms/token")
            print(f"   - P90: {decode_latency.get('p90', 0):.3f} ms/token")
            print(f"   - P99: {decode_latency.get('p99', 0):.3f} ms/token")
        
        if 'decoding' in perf_metrics and 'kv_transfer' in perf_metrics['decoding']:
            kv_transfer = perf_metrics['decoding']['kv_transfer']
            bandwidth = kv_transfer.get('bandwidth_blocks_per_sec', 0)
            results['kv_bandwidth_blocks_per_sec'] = bandwidth
            print(f"\nğŸ“¡ KVä¼ è¾“å¸¦å®½: {bandwidth:.1f} blocks/s")
            print(f"   - å¹³å‡ä¼ è¾“æ—¶é—´: {kv_transfer.get('avg_time_sec', 0)*1000:.2f} ms")
            print(f"   - å¹³å‡ä¼ è¾“å—æ•°: {kv_transfer.get('avg_blocks', 0):.1f} blocks")
        
        # ä¿å­˜ç»“æœ
        self.results['round1_baseline'] = results
        
        print("\nâœ… Round 1 å®Œæˆ!")
        print("[Round 1] Backendä¿æŒè¿è¡Œï¼Œå‡†å¤‡è¿›è¡Œworker roleåˆ‡æ¢...")
        
        # âœ… ä¿æŒbackendè¿è¡Œï¼Œä¸ºRound 2çš„roleåˆ‡æ¢åšå‡†å¤‡
        return results, backend  # å¤ç”¨backendè¿›è¡Œroleåˆ‡æ¢æµ‹è¯•
    
    async def round2_preemption(self, backend_from_round1=None):
        """
        Round 2: æŠ¢å æµ‹è¯•
        é…ç½®: 1E + 2P + 1D (é€šè¿‡worker roleåˆ‡æ¢å®ç°)
        è´Ÿè½½: 60 requests, æ³Šæ¾åˆ†å¸ƒ
        æµ‹é‡: ä¸Round 1å¯¹æ¯”ï¼Œè®¡ç®—prefillæ‰©å®¹å’Œdecodeç¼©å®¹çš„å½±å“
        """
        print("\n" + "="*80)
        print("ğŸ”¬ Round 2: æŠ¢å æµ‹è¯• (Preemption Simulation)")
        print("="*80)
        print("é…ç½®: 1 Encoding + 2 Prefill + 1 Decoding")
        print("è´Ÿè½½: 60 requests, æ³Šæ¾åˆ†å¸ƒ")
        print("ç›®æ ‡: æµ‹é‡æ‰©å®¹prefill/ç¼©å‡decodeå¯¹å»¶è¿Ÿçš„å½±å“")
        print("(ä¸Round 1çš„1E+1P+2Dé…ç½®å¯¹æ¯”)")
        print("="*80)
        
        # âœ… ä½¿ç”¨NCCLè§£è€¦æ¶æ„è¿›è¡ŒçœŸå®çš„worker roleåˆ‡æ¢
        if backend_from_round1 is None:
            raise RuntimeError("Round 2 requires backend from Round 1 for role switching test!")
        
        backend = backend_from_round1
        
        print("\n[Round 2] ğŸ”„ æ‰§è¡ŒWorker Roleåˆ‡æ¢: 1E+1P+2D â†’ 1E+2P+1D")
        print("  - Decoding Worker 1 (global_rank=3) â†’ Prefill Worker 1")
        print("  è¿™æ˜¯NCCLè§£è€¦æ¶æ„çš„å…³é”®æµ‹è¯•ï¼")
        
        # æ‰§è¡Œroleåˆ‡æ¢ï¼šå°†ä¸€ä¸ªdecode workeråˆ‡æ¢ä¸ºprefill worker
        # Round 1é…ç½®: encoding(0), prefill(1), decoding(2,3)
        # Round 2ç›®æ ‡: encoding(0), prefill(1,3), decoding(2)
        await backend.switch_worker_role(
            worker_id=1,  # decodingçš„ç¬¬2ä¸ªworker (global_rank=3)
            from_stage="decoding",
            to_stage="prefill",
            migrate_kv=True
        )
        
        print("[Round 2] âœ… Worker roleåˆ‡æ¢å®Œæˆ!")
        await asyncio.sleep(2)
        
        # åŠ è½½æµ‹è¯•å›¾ç‰‡
        print("\n[Round 2] Loading test images...")
        from PIL import Image
        image_base_path = "/root/lzd/dataset/shareGPT-4o/images"
        
        images = []
        for i in range(10, 30):
            image_path = f"{image_base_path}/{i}.jpg"
            try:
                img = Image.open(image_path)
                images.append(img)
            except:
                import numpy as np
                img_array = np.full((224, 224, 3), (128, 128, 128), dtype=np.uint8)
                img = Image.fromarray(img_array)
                images.append(img)
        
        # ç”Ÿæˆè¯·æ±‚
        print("\n[Round 2] Generating requests...")
        generator = PoissonRequestGenerator(arrival_rate=2.0, total_requests=60)
        requests = generator.create_test_requests(images)
        arrival_times = generator.generate_arrival_times()
        
        print(f"[Round 2] Generated {len(requests)} multimodal requests")
        
        # TODO: å®ç°æŠ¢å é€»è¾‘
        # åœ¨ç³»ç»Ÿè¿è¡Œåˆ°ä¸€å®šé˜¶æ®µæ—¶ï¼Œæ‰‹åŠ¨è§¦å‘PrefillæŠ¢å Decode worker
        
        start_time = time.time()
        completed_requests = []
        
        async def submit_requests():
            for i, (req, arrival_time) in enumerate(zip(requests, arrival_times)):
                current_time = time.time() - start_time
                wait_time = arrival_time - current_time
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
                
                await backend.add_request(req)
                print(f"[Round 2] Submitted {req.request_id} at t={time.time()-start_time:.2f}s")
        
        async def collect_outputs():
            while len(completed_requests) < len(requests):
                outputs = await backend.get_outputs()
                for output in outputs:
                    if output.finished:
                        completed_requests.append(output)
                        print(f"[Round 2] Completed {output.request_id} ({len(completed_requests)}/{len(requests)})")
                await asyncio.sleep(0.1)
        
        await asyncio.gather(
            submit_requests(),
            collect_outputs(),
        )
        
        total_time = time.time() - start_time
        print(f"\n[Round 2] All requests completed in {total_time:.2f}s")
        print(f"[Round 2] Throughput: {len(requests)/total_time:.2f} req/s")
        
        # è·å–æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        perf_metrics_final = backend.get_performance_metrics()
        
        results = {
            'config': '1E + 2P + 1D (simulating prefill scale-up)',
            'num_requests': len(requests),
            'total_time': total_time,
            'throughput': len(requests) / total_time,
            'metrics': perf_metrics_final,
        }
        
        # åˆ†ææŠ¢å å¯¹å»¶è¿Ÿçš„å½±å“
        print("\nğŸ“Š Round 2 æ€§èƒ½åˆ†æ:")
        print("="*60)
        
        # Decodeå»¶è¿Ÿåˆ†æ
        if 'round1_baseline' in self.results and 'decode_ms_per_token' in self.results['round1_baseline']:
            baseline_decode = self.results['round1_baseline']['decode_ms_per_token']
            if 'decoding' in perf_metrics_final and 'decode_latency_ms_per_token' in perf_metrics_final['decoding']:
                current_decode = perf_metrics_final['decoding']['decode_latency_ms_per_token']['mean']
                # æŠ¢å å¯¼è‡´decode workerå‡å°‘ï¼Œå»¶è¿Ÿåº”è¯¥å¢åŠ 
                slowdown_ratio = current_decode / baseline_decode if baseline_decode > 0 else 1.0
                results['decode_slowdown'] = slowdown_ratio
                results['preemption_cost_decode'] = (slowdown_ratio - 1.0) * 100  # ç™¾åˆ†æ¯”å¢åŠ 
                
                print(f"\nğŸ”´ Decodeå»¶è¿Ÿå˜åŒ– (Preemption Cost):")
                print(f"   - Baseline (1E+1P+2D): {baseline_decode:.3f} ms/token")
                print(f"   - After preemption (1E+2P+1D): {current_decode:.3f} ms/token")
                print(f"   - Slowdown: {slowdown_ratio:.3f}x")
                print(f"   - Cost: +{(slowdown_ratio-1.0)*100:.1f}% å»¶è¿Ÿå¢åŠ ")
        
        # Prefillå»¶è¿Ÿåˆ†æ
        if 'prefill' in perf_metrics_final and 'prefill_latency_ms_per_token' in perf_metrics_final['prefill']:
            prefill_latency = perf_metrics_final['prefill']['prefill_latency_ms_per_token']['mean']
            results['prefill_ms_per_token'] = prefill_latency
            
            # å¦‚æœRound 1æœ‰prefillæ•°æ®ï¼Œå¯¹æ¯”
            if 'round1_baseline' in self.results and 'metrics' in self.results['round1_baseline']:
                r1_metrics = self.results['round1_baseline']['metrics']
                if 'prefill' in r1_metrics and 'prefill_latency_ms_per_token' in r1_metrics['prefill']:
                    baseline_prefill = r1_metrics['prefill']['prefill_latency_ms_per_token']['mean']
                    # æŠ¢å å¢åŠ äº†prefill workerï¼Œå»¶è¿Ÿåº”è¯¥å‡å°‘
                    speedup_ratio = baseline_prefill / prefill_latency if prefill_latency > 0 else 1.0
                    results['prefill_speedup'] = speedup_ratio
                    results['preemption_gain_prefill'] = (speedup_ratio - 1.0) * 100  # ç™¾åˆ†æ¯”å‡å°‘
                    
                    print(f"\nğŸŸ¢ Prefillå»¶è¿Ÿå˜åŒ– (Preemption Gain):")
                    print(f"   - Baseline (1E+1P+2D): {baseline_prefill:.3f} ms/token")
                    print(f"   - After preemption (1E+2P+1D): {prefill_latency:.3f} ms/token")
                    print(f"   - Speedup: {speedup_ratio:.3f}x")
                    print(f"   - Gain: {(speedup_ratio-1.0)*100:.1f}% å»¶è¿Ÿå‡å°‘")
            else:
                print(f"\nğŸŸ¢ Prefillå»¶è¿Ÿ (å½’ä¸€åŒ–): {prefill_latency:.3f} ms/token")
        
        print("="*60)
        
        self.results['round2_preemption'] = results
        
        print("\nâœ… Round 2 å®Œæˆ!")
        print("ğŸ’¡ Round 2ç»“æŸé…ç½®ä¸º 1E+2P+1Dï¼Œä¸Round 3ç›¸åŒï¼Œå¤ç”¨backend...")
        
        # ä¸åœæ­¢backendï¼Œç›´æ¥è¿”å›ç»™Round 3ä½¿ç”¨
        return results, backend  # è¿”å›backendä¾›Round 3å¤ç”¨
    
    async def round3_peak_stress(self, backend=None):
        """
        Round 3: å³°å€¼å‹æµ‹
        é…ç½®: 1E + 2P + 1D (å¯ä»¥å¤ç”¨Round 2çš„backend)
        è´Ÿè½½: å³°å€¼è´Ÿè½½ï¼Œæ‰¾åˆ°decodeæœ€å¤§token budget
        æµ‹è¯•: æŠ¢å 1ä¸ªprefill workerï¼Œæµ‹é‡æé™æƒ…å†µä¸‹çš„gain/cost
        
        Args:
            backend: å¦‚æœæä¾›ï¼Œåˆ™å¤ç”¨ç°æœ‰backendï¼ˆæ¥è‡ªRound 2ï¼‰
        """
        print("\n" + "="*80)
        print("ğŸ”¬ Round 3: å³°å€¼å‹æµ‹ (Peak Stress Test)")
        print("="*80)
        print("é…ç½®: 1 Encoding + 2 Prefill + 1 Decoding")
        print("è´Ÿè½½: å³°å€¼è´Ÿè½½ï¼Œæµ‹è¯•æœ€å¤§token budget")
        print("ç›®æ ‡: æ‰¾åˆ°decodeç“¶é¢ˆï¼Œæµ‹è¯•æé™æŠ¢å åœºæ™¯")
        print("="*80)
        
        # å¦‚æœæ²¡æœ‰æä¾›backendï¼Œåˆ™åˆ›å»ºæ–°çš„
        backend_created = False
        if backend is None:
            print("\n[Round 3] Creating new backend...")
            backend = V0EngineBackend(
                model_path=self.model_path,
                num_encoding_workers=1,
                num_prefill_workers=2,
                num_decoding_workers=1,
                block_size=16,
                max_num_gpu_blocks=3000,
                dtype="float16",
                gpu_memory_utilization=0.85,
                kv_transfer_method="nccl",
                limit_mm_per_prompt={"image": 1},
            )
            
            print("[Round 3] Initializing backend...")
            await backend.initialize()
            
            print("[Round 3] Starting backend...")
            await backend.start()
            await asyncio.sleep(2)
            backend_created = True
        else:
            print("\n[Round 3] âœ… Reusing backend from Round 2 (smart!)")
            # ç­‰å¾…ä¸€ä¸‹ï¼Œè®©ç³»ç»Ÿç¨³å®š
            await asyncio.sleep(1)
        
        # åŠ è½½æµ‹è¯•å›¾ç‰‡
        print("\n[Round 3] Loading test images...")
        from PIL import Image
        image_base_path = "/root/lzd/dataset/shareGPT-4o/images"
        
        images = []
        for i in range(10, 30):
            image_path = f"{image_base_path}/{i}.jpg"
            try:
                img = Image.open(image_path)
                images.append(img)
            except:
                import numpy as np
                img_array = np.full((224, 224, 3), (128, 128, 128), dtype=np.uint8)
                img = Image.fromarray(img_array)
                images.append(img)
        
        # ç”Ÿæˆå¤§é‡è¯·æ±‚ï¼Œæ¨¡æ‹Ÿå³°å€¼è´Ÿè½½
        print("\n[Round 3] Generating peak load requests...")
        num_requests = 100
        max_tokens_per_req = 50  # æ›´é•¿çš„è¾“å‡º
        
        prompts = [
            "<|user|>\n<|vision_start|><|image_pad|><|vision_end|>\nè¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚\n<|assistant|>\n",
            "<|user|>\n<|vision_start|><|image_pad|><|vision_end|>\nå›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ\n<|assistant|>\n",
            "<|user|>\n<|vision_start|><|image_pad|><|vision_end|>\nè¿™æ˜¯ä»€ä¹ˆåœºæ™¯ï¼Ÿ\n<|assistant|>\n",
        ]
        
        requests = []
        for i in range(num_requests):
            img = images[i % len(images)]
            prompt = prompts[i % len(prompts)]
            
            req = Request(
                request_id=f"peak_req_{i:03d}",
                prompt=prompt,
                max_tokens=max_tokens_per_req,
                temperature=0.8,
                top_p=0.9,
                multi_modal_data={"image": img},
            )
            requests.append(req)
        
        print(f"[Round 3] Generated {len(requests)} multimodal requests (longer sequences)")
        
        # å¿«é€Ÿæäº¤æ‰€æœ‰è¯·æ±‚ (æ¨¡æ‹Ÿçªå‘è´Ÿè½½)
        start_time = time.time()
        print("\n[Round 3] Submitting all requests rapidly...")
        for i, req in enumerate(requests):
            await backend.add_request(req)
            if i % 10 == 0:
                print(f"[Round 3] Submitted {i+1}/{len(requests)} requests")
                await asyncio.sleep(0.1)  # è½»å¾®å»¶è¿Ÿï¼Œé¿å…è¿‡è½½
        
        print(f"[Round 3] All requests submitted at t={time.time()-start_time:.2f}s")
        
        # ç›‘æ§ç³»ç»ŸçŠ¶æ€å’Œdecode batch capacity
        completed_requests = []
        request_token_counts = {}  # è®°å½•æ¯ä¸ªè¯·æ±‚ç”Ÿæˆçš„tokenæ•°
        max_decode_batch_tokens = 0  # è®°å½•decode batchä¸­çš„æœ€å¤§æ€»tokenæ•°
        
        while len(completed_requests) < len(requests):
            outputs = await backend.get_outputs()
            for output in outputs:
                # ç´¯åŠ æ¯ä¸ªè¯·æ±‚çš„tokenæ•°
                if output.request_id not in request_token_counts:
                    request_token_counts[output.request_id] = 0
                request_token_counts[output.request_id] += len(output.output_token_ids)
                
                if output.finished:
                    completed_requests.append(output)
            
            # æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
            stats = backend.get_stats()
            decode_running = stats['decoding']['num_running']
            decode_waiting = stats['decoding']['num_waiting']
            
            # è®¡ç®—å½“å‰decode batchä¸­çš„æ€»tokenæ•°
            # è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼å€¼ï¼šrunningè¯·æ±‚æ•° * å¹³å‡æ¯ä¸ªè¯·æ±‚çš„å½“å‰tokenæ•°
            if decode_running > 0:
                # è·å–å½“å‰æ­£åœ¨decodeçš„è¯·æ±‚çš„å¹³å‡tokenæ•°
                running_request_tokens = []
                for req_id, token_count in request_token_counts.items():
                    # æ£€æŸ¥è¿™ä¸ªè¯·æ±‚æ˜¯å¦è¿˜åœ¨runningï¼ˆæœªå®Œæˆï¼‰
                    if req_id not in [out.request_id for out in completed_requests]:
                        running_request_tokens.append(token_count)
                
                if running_request_tokens:
                    current_batch_tokens = sum(running_request_tokens)
                    max_decode_batch_tokens = max(max_decode_batch_tokens, current_batch_tokens)
                    
                    if len(completed_requests) % 20 == 0 and len(completed_requests) > 0:
                        print(f"[Round 3] Current decode batch: {decode_running} reqs, "
                              f"~{current_batch_tokens} tokens total (max so far: {max_decode_batch_tokens})")
            
            
            await asyncio.sleep(0.1)
        
        total_time = time.time() - start_time
        print(f"\n[Round 3] All requests completed in {total_time:.2f}s")
        print(f"[Round 3] Throughput: {len(requests)/total_time:.2f} req/s")
        
        # è·å–æ€§èƒ½æŒ‡æ ‡
        stats = backend.get_stats()
        
        # è®¡ç®—æ€»decode tokensï¼šä»request_token_countsç»Ÿè®¡
        total_tokens_decode = sum(request_token_counts.values())
        
        # ä»decode metricsä¸­è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
        decode_metrics = stats.get('decoding_metrics', {})
        
        results = {
            'config': '1E + 2P + 1D (peak load)',
            'num_requests': len(requests),
            'total_time': total_time,
            'throughput': len(requests) / total_time,
            'total_decode_tokens': total_tokens_decode,
            'max_decode_batch_tokens': max_decode_batch_tokens,  # è¿™æ‰æ˜¯ä½ è¦çš„token budgetï¼
            'decode_metrics': decode_metrics,
        }
        
        print(f"\nğŸ“Š Peak Load Results:")
        print(f"   - Total decode tokens generated: {total_tokens_decode}")
        print(f"   - Max decode batch capacity: {max_decode_batch_tokens} tokens (å³°å€¼å¹¶å‘)")
        print(f"   - Avg tokens per request: {total_tokens_decode/len(requests):.1f}")
        print(f"   - Throughput: {total_tokens_decode/total_time:.1f} tokens/s")
        
        self.results['round3_peak'] = results
        
        # åªåœ¨æˆ‘ä»¬åˆ›å»ºäº†æ–°backendæ—¶æ‰åœæ­¢å®ƒ
        if backend_created:
            await backend.stop()
        else:
            # å¦‚æœæ˜¯å¤ç”¨çš„backendï¼Œç°åœ¨åœæ­¢å®ƒ
            await backend.stop()
        
        print("\nâœ… Round 3 å®Œæˆ!")
        return results
    
    def compute_final_parameters(self):
        """æ ¹æ®ä¸‰è½®æµ‹è¯•ç»“æœï¼Œè®¡ç®—æœ€ç»ˆçš„Gain-Costå‚æ•°"""
        print("\n" + "="*80)
        print("ğŸ“Š è®¡ç®—æœ€ç»ˆå‚æ•°")
        print("="*80)
        
        params = {}
        
        # ===== åŸºç¡€å»¶è¿Ÿ (ä»Round 1) =====
        print("\n1ï¸âƒ£ åŸºç¡€å»¶è¿Ÿ (Round 1 Baseline):")
        r1_metrics = self.results.get('round1_baseline', {}).get('metrics', {})
        
        # Encodingå»¶è¿Ÿ
        if 'encoding' in r1_metrics and 'encoding_latency_ms_per_token' in r1_metrics['encoding']:
            enc_latency = r1_metrics['encoding']['encoding_latency_ms_per_token']['mean']
            params['encoding_latency_ms_per_token'] = enc_latency
            print(f"   âœ“ Encoding: {enc_latency:.3f} ms/token")
        else:
            params['encoding_latency_ms_per_token'] = 10.0  # é»˜è®¤å€¼
            print(f"   âš ï¸  Encoding: ä½¿ç”¨é»˜è®¤å€¼ 10.0 ms/token")
        
        # Prefillå»¶è¿Ÿ
        if 'prefill' in r1_metrics and 'prefill_latency_ms_per_token' in r1_metrics['prefill']:
            prefill_latency = r1_metrics['prefill']['prefill_latency_ms_per_token']['mean']
            params['prefill_latency_ms_per_token'] = prefill_latency
            print(f"   âœ“ Prefill: {prefill_latency:.3f} ms/token")
        else:
            params['prefill_latency_ms_per_token'] = 5.0  # é»˜è®¤å€¼
            print(f"   âš ï¸  Prefill: ä½¿ç”¨é»˜è®¤å€¼ 5.0 ms/token")
        
        # Decodeå»¶è¿Ÿ
        if 'decoding' in r1_metrics and 'decode_latency_ms_per_token' in r1_metrics['decoding']:
            decode_latency = r1_metrics['decoding']['decode_latency_ms_per_token']['mean']
            params['decode_latency_ms_per_token'] = decode_latency
            print(f"   âœ“ Decode: {decode_latency:.3f} ms/token")
        else:
            params['decode_latency_ms_per_token'] = 20.0  # é»˜è®¤å€¼
            print(f"   âš ï¸  Decode: ä½¿ç”¨é»˜è®¤å€¼ 20.0 ms/token")
        
        # ===== Migration Cost (ä»KVä¼ è¾“æ•°æ®è®¡ç®—) =====
        print("\n2ï¸âƒ£ Migration Cost (KVä¼ è¾“å¼€é”€):")
        
        # å°è¯•ä»Round 2å’ŒRound 3ä¸­æå–KVä¼ è¾“æ•°æ®ï¼ˆè¿™äº›è½®æ¬¡æœ‰è¿ç§»ï¼‰
        kv_transfer_found = False
        for round_name in ['round2_preemption', 'round3_peak', 'round1_baseline']:
            round_data = self.results.get(round_name, {})
            round_metrics = round_data.get('metrics', {})
            
            if 'decoding' in round_metrics and 'kv_transfer' in round_metrics['decoding']:
                kv_transfer = round_metrics['decoding']['kv_transfer']
                avg_time = kv_transfer.get('avg_time_sec', 0)
                avg_blocks = kv_transfer.get('avg_blocks', 0)
                bandwidth = kv_transfer.get('bandwidth_blocks_per_sec', 0)
                
                # ä½¿ç”¨å¹³å‡ä¼ è¾“æ—¶é—´ä½œä¸ºè¿ç§»å¼€é”€
                if avg_time > 0:
                    params['migration_cost'] = avg_time
                    params['kv_transfer_bandwidth'] = bandwidth
                    print(f"   âœ“ Migration Cost: {avg_time:.4f}s (from {round_name})")
                    print(f"      å¹³å‡ä¼ è¾“ {avg_blocks:.1f} blocks")
                    print(f"      KVä¼ è¾“å¸¦å®½: {bandwidth:.1f} blocks/s")
                    kv_transfer_found = True
                    break
        
        if not kv_transfer_found:
            params['migration_cost'] = 0.01  # é»˜è®¤å€¼
            params['kv_transfer_bandwidth'] = 0
            print(f"   âš ï¸  Migration Cost: ä½¿ç”¨é»˜è®¤å€¼ 0.01s (æ— KVä¼ è¾“æ•°æ®)")
        
        # ===== Preemption Penalty (ä»Round 2è®¡ç®—) =====
        print("\n3ï¸âƒ£ Preemption Penalty (æŠ¢å æƒ©ç½šå› å­):")
        r2_results = self.results.get('round2_preemption', {})
        
        if 'preemption_cost_decode' in r2_results or 'decode_slowdown' in r2_results:
            # ä»decodeå»¶è¿Ÿå¢åŠ è®¡ç®—æƒ©ç½š
            slowdown = r2_results.get('decode_slowdown', 1.0)
            cost_pct = r2_results.get('preemption_cost_decode', 0)
            
            # æƒ©ç½šå› å­ = å»¶è¿Ÿå¢åŠ æ¯”ä¾‹ (ä½œä¸ºä¹˜æ•°å› å­ï¼Œä¸æ˜¯ç»å¯¹æ—¶é—´)
            penalty_factor = slowdown - 1.0  # ä¾‹å¦‚1.2x slowdown => 0.2 penalty factor
            params['preemption_penalty'] = max(penalty_factor, 0.0)
            print(f"   âœ“ Preemption Penalty: {penalty_factor:.3f} (slowdown={slowdown:.3f}x, cost=+{cost_pct:.1f}%)")
        else:
            params['preemption_penalty'] = 0.2  # é»˜è®¤20%æƒ©ç½š
            print(f"   âš ï¸  Preemption Penalty: ä½¿ç”¨é»˜è®¤å€¼ 0.2 (20% slowdown)")
        
        # ===== Scalability Coefficients =====
        print("\n4ï¸âƒ£ Scalability Coefficients:")
        params['scalability_encode'] = 0.80
        params['scalability_prefill'] = 0.90
        params['scalability_decoding'] = 0.75
        print(f"   âœ“ Encode: {params['scalability_encode']}")
        print(f"   âœ“ Prefill: {params['scalability_prefill']}")
        print(f"   âœ“ Decode: {params['scalability_decoding']}")
        
        # ===== Max Token Budget (ä»Round 3) =====
        print("\n5ï¸âƒ£ Max Decode Token Budget:")
        if 'max_decode_batch_tokens' in self.results.get('round3_peak', {}):
            max_budget = self.results['round3_peak']['max_decode_batch_tokens']
            params['max_decode_token_budget'] = int(max_budget)
            print(f"   âœ“ Max Decode Token Budget: {int(max_budget)} tokens (peak batch capacity)")
        else:
            params['max_decode_token_budget'] = 2000  # é»˜è®¤å€¼
            print(f"   âš ï¸  Max Decode Token Budget: ä½¿ç”¨é»˜è®¤å€¼ 2000 tokens")
        
        self.results['final_params'] = params
        
        print("\n" + "="*80)
        print("ğŸ¯ æœ€ç»ˆæ¨èå‚æ•°:")
        print("="*80)
        print(f"  encoding_latency_ms_per_token: {params.get('encoding_latency_ms_per_token', 0):.3f}")
        print(f"  prefill_latency_ms_per_token: {params.get('prefill_latency_ms_per_token', 0):.3f}")
        print(f"  decode_latency_ms_per_token: {params.get('decode_latency_ms_per_token', 0):.3f}")
        print(f"  migration_cost: {params.get('migration_cost', 0):.4f}")
        print(f"  preemption_penalty: {params.get('preemption_penalty', 0):.3f}")
        print(f"  scalability_encode: {params.get('scalability_encode', 0)}")
        print(f"  scalability_prefill: {params.get('scalability_prefill', 0)}")
        print(f"  scalability_decoding: {params.get('scalability_decoding', 0)}")
        print(f"  max_decode_token_budget: {params.get('max_decode_token_budget', 0)}")
        
        return params
    
    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœåˆ°JSONæ–‡ä»¶"""
        # ä¿å­˜ä¸ºç³»ç»Ÿé…ç½®æ–‡ä»¶
        config_file = self.output_dir / "gain_cost_params.json"
        with open(config_file, 'w') as f:
            json.dump(self.results['final_params'], f, indent=2)
        print(f"\nğŸ’¾ Gain-Cost parameters saved to {config_file}")
        print(f"    System will load this file on startup.")
        
        # ä¿å­˜è¯¦ç»†çš„æ ¡å‡†æ•°æ®ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
        detail_file = self.output_dir / "calibration_details.json"
        with open(detail_file, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"ğŸ’¾ Detailed calibration data saved to {detail_file}")
        
        # ç”ŸæˆPythonä»£ç ç‰‡æ®µ
        code_file = self.output_dir / "recommended_params.py"
        params = self.results['final_params']
        code = f'''# Gain-Cost Model Parameters (Calibrated on {time.strftime("%Y-%m-%d %H:%M:%S")})
# Hardware: {self.model_path}

# Base latencies (ms per token)
ENCODING_LATENCY_MS_PER_TOKEN = {params.get('encoding_latency_ms_per_token', 10.0):.3f}
PREFILL_LATENCY_MS_PER_TOKEN = {params.get('prefill_latency_ms_per_token', 5.0):.3f}
DECODE_LATENCY_MS_PER_TOKEN = {params.get('decode_latency_ms_per_token', 20.0):.3f}

# Migration cost (seconds per migration operation)
MIGRATION_COST = {params.get('migration_cost', 0.01):.4f}

# Preemption penalty (slowdown factor, e.g., 0.2 = 20% slowdown)
PREEMPTION_PENALTY = {params.get('preemption_penalty', 0.2):.3f}

# Scalability coefficients (parallel efficiency)
SCALABILITY_ENCODE = {params.get('scalability_encode', 0.80):.2f}
SCALABILITY_PREFILL = {params.get('scalability_prefill', 0.90):.2f}
SCALABILITY_DECODING = {params.get('scalability_decoding', 0.75):.2f}

# Max token budget (total tokens in decode batch at capacity)
MAX_DECODE_TOKEN_BUDGET = {params.get('max_decode_token_budget', 2000)}
'''
        with open(code_file, 'w') as f:
            f.write(code)
        print(f"ğŸ’¾ Python code saved to {code_file}")
    
    async def run_all(self):
        """è¿è¡Œæ‰€æœ‰ä¸‰è½®æµ‹è¯•"""
        print("\n" + "="*80)
        print("ğŸš€ Gain-Cost Model Calibration v2")
        print("="*80)
        print(f"Model: {self.model_path}")
        print(f"Output directory: {self.output_dir}")
        print("="*80)
        
        try:
            # Round 1: Baseline (è¿”å›backendä¾›Round 2å¤ç”¨)
            round1_results, round1_backend = await self.round1_baseline()
            
            # Round 2: Preemption (å¤ç”¨Round 1çš„backendï¼Œè¿”å›backendä¾›Round 3å¤ç”¨)
            round2_results, round2_backend = await self.round2_preemption(backend_from_round1=round1_backend)
            
            # Round 3: Peak stress (å¤ç”¨Round 2çš„backend)
            await self.round3_peak_stress(backend=round2_backend)
            
            # Compute final parameters
            self.compute_final_parameters()
            
            # Save results
            self.save_results()
            
            print("\n" + "="*80)
            print("âœ… æ ¡å‡†å®Œæˆï¼")
            print("="*80)
            
        except Exception as e:
            print(f"\nâŒ Error during calibration: {e}")
            import traceback
            traceback.print_exc()


async def main():
    parser = argparse.ArgumentParser(description="Gain-Cost Model Calibration v2")
    parser.add_argument("--model_path", type=str, 
                        default="/root/lzd/model/qwen2.5-VL",
                        help="Path to model")
    parser.add_argument("--output_dir", type=str,
                        default="./calibration_results",
                        help="Output directory for results")
    
    args = parser.parse_args()
    
    calibrator = GainCostCalibrator(
        model_path=args.model_path,
        output_dir=args.output_dir,
    )
    
    await calibrator.run_all()


if __name__ == "__main__":
    asyncio.run(main())

