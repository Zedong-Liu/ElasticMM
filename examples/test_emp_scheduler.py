"""
æµ‹è¯•EMPSchedulerçš„å¼¹æ€§è°ƒåº¦åŠŸèƒ½
éªŒè¯ï¼š
1. get_worker_allocation() API
2. å¼¹æ€§è°ƒåº¦å¾ªç¯
3. å†å²æ•°æ®æ”¶é›†
4. èµ„æºé‡æ–°å¹³è¡¡
"""

import asyncio
import time
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from elasticmm.engine.v0 import V0EngineBackend
from elasticmm.core.scheduler import EMPScheduler
from elasticmm.engine.v0.utils import Request
from elasticmm.core.balancer import ModalityType


async def test_basic_apis():
    """æµ‹è¯•åŸºç¡€API"""
    print("\n" + "="*80)
    print("æµ‹è¯•1: åŸºç¡€API")
    print("="*80)
    
    # åˆ›å»ºbackend (1E + 1P + 2D)
    backend = V0EngineBackend(
        model_path="/root/lzd/model/qwen2.5-VL",
        num_encoding_workers=1,
        num_prefill_workers=1,
        num_decoding_workers=2,
        block_size=16,
        max_num_gpu_blocks=3000,
        dtype="float16",
        gpu_memory_utilization=0.85,
        kv_transfer_method="nccl",
        limit_mm_per_prompt={"image": 1},
    )
    
    print("\n[Test] Initializing backend...")
    await backend.initialize()
    
    # æµ‹è¯•get_worker_allocation
    print("\n[Test] Testing get_worker_allocation()...")
    worker_alloc = backend.get_worker_allocation()
    print(f"âœ“ Worker allocation: {worker_alloc}")
    
    expected = {0: 'encoding', 1: 'prefill', 2: 'decoding', 3: 'decoding'}
    assert worker_alloc == expected, f"Expected {expected}, got {worker_alloc}"
    print("âœ“ Worker allocation API works correctly!")
    
    # æµ‹è¯•get_stats
    print("\n[Test] Testing get_stats()...")
    stats = backend.get_stats()
    print(f"âœ“ Stats: P({stats['prefill']['num_workers']}w) "
          f"D({stats['decoding']['num_workers']}w)")
    
    await backend.stop()
    print("\nâœ… Test 1 passed!")
    return True


async def test_scheduler_integration():
    """æµ‹è¯•è°ƒåº¦å™¨é›†æˆ"""
    print("\n" + "="*80)
    print("æµ‹è¯•2: EMPScheduleré›†æˆ")
    print("="*80)
    
    # åˆ›å»ºbackend
    backend = V0EngineBackend(
        model_path="/root/lzd/model/qwen2.5-VL",
        num_encoding_workers=1,
        num_prefill_workers=1,
        num_decoding_workers=2,
        block_size=16,
        max_num_gpu_blocks=3000,
        dtype="float16",
        gpu_memory_utilization=0.85,
        kv_transfer_method="nccl",
        limit_mm_per_prompt={"image": 1},
    )
    
    print("\n[Test] Initializing backend...")
    await backend.initialize()
    await backend.start()
    
    # åˆ›å»ºscheduler
    print("\n[Test] Creating EMPScheduler...")
    scheduler = EMPScheduler(backend=backend)
    
    # æµ‹è¯•ç»§æ‰¿
    print("\n[Test] Testing inheritance from Scheduler...")
    assert hasattr(scheduler, 'heartbeat'), "Should inherit heartbeat()"
    assert hasattr(scheduler, 'select_prefill'), "Should inherit select_prefill()"
    print("âœ“ EMPScheduler correctly inherits from Scheduler")
    
    # æµ‹è¯•æ–°åŠŸèƒ½
    print("\n[Test] Testing new features...")
    assert hasattr(scheduler, 'start_elastic_scheduling'), "Should have elastic scheduling"
    assert hasattr(scheduler, '_elastic_scheduling_loop'), "Should have scheduling loop"
    print("âœ“ New elastic scheduling methods exist")
    
    # æµ‹è¯•allocatorå†å²æ•°æ®
    print("\n[Test] Testing allocator history...")
    from elasticmm.core.allocator import InferenceStage
    allocator = scheduler.stage_allocators[ModalityType.TEXT_ONLY]
    
    assert hasattr(allocator, 'workload_history'), "Should have workload_history"
    assert hasattr(allocator, 'record_step_stats'), "Should have record_step_stats"
    
    # è®°å½•ä¸€äº›æ•°æ®
    allocator.record_step_stats(InferenceStage.PREFILL, 10)
    allocator.record_step_stats(InferenceStage.PREFILL, 15)
    allocator.record_step_stats(InferenceStage.PREFILL, 12)
    
    assert len(allocator.workload_history[InferenceStage.PREFILL]) == 3
    print("âœ“ Workload history recording works")
    
    # æµ‹è¯•é¢„æµ‹
    estimated = allocator._estimate_future_workload(InferenceStage.PREFILL, 20)
    print(f"âœ“ Future workload estimation: {estimated} (current: 20)")
    
    await backend.stop()
    print("\nâœ… Test 2 passed!")
    return True


async def test_elastic_scheduling_short():
    """æµ‹è¯•å¼¹æ€§è°ƒåº¦ï¼ˆçŸ­æ—¶é—´è¿è¡Œï¼‰"""
    print("\n" + "="*80)
    print("æµ‹è¯•3: å¼¹æ€§è°ƒåº¦å¾ªç¯ï¼ˆ30ç§’æµ‹è¯•ï¼‰")
    print("="*80)
    
    # åˆ›å»ºbackend
    backend = V0EngineBackend(
        model_path="/root/lzd/model/qwen2.5-VL",
        num_encoding_workers=1,
        num_prefill_workers=1,
        num_decoding_workers=2,
        block_size=16,
        max_num_gpu_blocks=3000,
        dtype="float16",
        gpu_memory_utilization=0.85,
        kv_transfer_method="nccl",
        limit_mm_per_prompt={"image": 1},
    )
    
    print("\n[Test] Initializing backend...")
    await backend.initialize()
    await backend.start()
    
    # åˆ›å»ºschedulerå¹¶å¯åŠ¨å¼¹æ€§è°ƒåº¦
    print("\n[Test] Starting elastic scheduling...")
    scheduler = EMPScheduler(backend=backend)
    scheduler.start_elastic_scheduling()
    
    print("âœ“ Elastic scheduling loop started")
    print("  Monitoring for 30 seconds...")
    
    # æäº¤ä¸€äº›è¯·æ±‚æ¨¡æ‹Ÿworkload
    print("\n[Test] Submitting test requests...")
    from PIL import Image
    import numpy as np
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    img_array = np.full((224, 224, 3), (128, 128, 128), dtype=np.uint8)
    test_image = Image.fromarray(img_array)
    
    # æäº¤10ä¸ªè¯·æ±‚
    for i in range(10):
        request = Request(
            request_id=f"test_req_{i}",
            prompt="Describe this image.",
            max_tokens=20,
            multi_modal_data={"image": [test_image]},
        )
        await backend.add_request(request)
        print(f"  Submitted request {i+1}/10")
        await asyncio.sleep(0.5)
    
    # ç›‘æ§30ç§’
    start_time = time.time()
    check_count = 0
    
    while time.time() - start_time < 30:
        await asyncio.sleep(5)
        check_count += 1
        
        # æ£€æŸ¥çŠ¶æ€
        stats = backend.get_stats()
        worker_alloc = backend.get_worker_allocation()
        allocator = scheduler.stage_allocators[ModalityType.TEXT_ONLY]
        
        print(f"\n[Check {check_count}] Status:")
        print(f"  Worker allocation: {worker_alloc}")
        print(f"  Prefill: {stats['prefill']['num_workers']}w, "
              f"{stats['prefill']['num_waiting']}q")
        print(f"  Decoding: {stats['decoding']['num_workers']}w, "
              f"{stats['decoding']['num_waiting']}q")
        print(f"  Step counter: {allocator.step_counter}")
        
        # æ£€æŸ¥å†å²æ•°æ®
        from elasticmm.core.allocator import InferenceStage
        history_len = len(allocator.workload_history[InferenceStage.PREFILL])
        if history_len > 0:
            print(f"  History collected: {history_len} samples")
    
    print("\n[Test] Stopping elastic scheduling...")
    await scheduler.stop_elastic_scheduling()
    
    # éªŒè¯
    allocator = scheduler.stage_allocators[ModalityType.TEXT_ONLY]
    assert allocator.step_counter > 0, "Step counter should have incremented"
    print(f"âœ“ Step counter incremented to: {allocator.step_counter}")
    
    # æ£€æŸ¥å†å²æ•°æ®
    from elasticmm.core.allocator import InferenceStage
    history = allocator.workload_history[InferenceStage.PREFILL]
    if len(history) > 0:
        print(f"âœ“ Collected {len(history)} workload samples")
        stats = allocator.get_workload_stats(InferenceStage.PREFILL)
        print(f"  Workload stats: mean={stats['mean']:.1f}, "
              f"trend={stats['trend']:.2f}")
    
    await backend.stop()
    print("\nâœ… Test 3 passed!")
    return True


async def test_worker_migration():
    """æµ‹è¯•workerè¿ç§»åŠŸèƒ½"""
    print("\n" + "="*80)
    print("æµ‹è¯•4: Workerè¿ç§»")
    print("="*80)
    
    # åˆ›å»ºbackend (1E + 1P + 2D)
    backend = V0EngineBackend(
        model_path="/root/lzd/model/qwen2.5-VL",
        num_encoding_workers=1,
        num_prefill_workers=1,
        num_decoding_workers=2,
        block_size=16,
        max_num_gpu_blocks=3000,
        dtype="float16",
        gpu_memory_utilization=0.85,
        kv_transfer_method="nccl",
        limit_mm_per_prompt={"image": 1},
    )
    
    print("\n[Test] Initializing backend...")
    await backend.initialize()
    await backend.start()
    
    # åˆå§‹åˆ†é…
    print("\n[Test] Initial allocation:")
    alloc_before = backend.get_worker_allocation()
    print(f"  {alloc_before}")
    stats_before = backend.get_stats()
    print(f"  Prefill: {stats_before['prefill']['num_workers']}w, "
          f"Decoding: {stats_before['decoding']['num_workers']}w")
    
    # æ‰§è¡Œè¿ç§»: worker 2 (decoding -> prefill)
    print("\n[Test] Migrating worker 2: decoding -> prefill")
    success = await backend.switch_worker_role(
        worker_id=2,
        from_stage='decoding',
        to_stage='prefill',
        migrate_kv=True
    )
    
    if success:
        print("âœ“ Migration successful!")
        
        # éªŒè¯æ–°åˆ†é…
        await asyncio.sleep(2)  # ç­‰å¾…è¿ç§»å®Œæˆ
        alloc_after = backend.get_worker_allocation()
        print(f"\n[Test] New allocation:")
        print(f"  {alloc_after}")
        
        stats_after = backend.get_stats()
        print(f"  Prefill: {stats_after['prefill']['num_workers']}w, "
              f"Decoding: {stats_after['decoding']['num_workers']}w")
        
        # éªŒè¯å˜åŒ–
        assert alloc_after[2] == 'prefill', "Worker 2 should be prefill now"
        assert stats_after['prefill']['num_workers'] == 2, "Prefill should have 2 workers"
        assert stats_after['decoding']['num_workers'] == 1, "Decoding should have 1 worker"
        
        print("âœ“ Worker allocation updated correctly!")
    else:
        print("âŒ Migration failed")
        return False
    
    await backend.stop()
    print("\nâœ… Test 4 passed!")
    return True


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*80)
    print("ElasticMM Scheduler Integration Tests")
    print("="*80)
    
    import ray
    ray.init(ignore_reinit_error=True)
    
    try:
        # Test 1: åŸºç¡€API
        result1 = await test_basic_apis()
        await asyncio.sleep(3)
        
        # Test 2: è°ƒåº¦å™¨é›†æˆ
        result2 = await test_scheduler_integration()
        await asyncio.sleep(3)
        
        # Test 3: å¼¹æ€§è°ƒåº¦å¾ªç¯ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
        result3 = await test_elastic_scheduling_short()
        await asyncio.sleep(3)
        
        # Test 4: Workerè¿ç§»
        result4 = await test_worker_migration()
        
        # æ€»ç»“
        print("\n" + "="*80)
        print("æµ‹è¯•æ€»ç»“")
        print("="*80)
        print(f"Test 1 (Basic APIs): {'âœ… PASS' if result1 else 'âŒ FAIL'}")
        print(f"Test 2 (Scheduler Integration): {'âœ… PASS' if result2 else 'âŒ FAIL'}")
        print(f"Test 3 (Elastic Scheduling): {'âœ… PASS' if result3 else 'âŒ FAIL'}")
        print(f"Test 4 (Worker Migration): {'âœ… PASS' if result4 else 'âŒ FAIL'}")
        
        if all([result1, result2, result3, result4]):
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            return 0
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return 1
    
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        ray.shutdown()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)


